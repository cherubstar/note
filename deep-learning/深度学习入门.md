---
title: 深度学习入门
tags: 新建,模板,小书匠
grammar_cjkRuby: true
---


欢迎使用 **{小书匠}(xiaoshujiang)编辑器**，您可以通过 `小书匠主按钮>模板` 里的模板管理来改变新建文章的内容。

## 一、深度学习与人工智能简介




## 二、
#### 1、图像在计算机中长什么样呢


## 三、K - 近邻
![](./images/1574067191154.png)

----------
>**概述**
~~~
	KNN 算法本身简单有效，它是一种 lazy-learning 算法,分类器不需要使用训练集进行训练，训练时间复杂度为 0。KNN 分类的计算复杂度和训练集中的文档数目成正比，也就是说，如果训练集中文档总数为 n,那么 KNN 的分类时间复杂度为 O(n)。
~~~
>__对于未知类别属性数据集中的点__
```
	1.计算已知类别数据集中的点与当前点的距离
	2.按照距离一次排序
	3.选取与当前点距离最小的K个点
	4.确定前K个点所在类别的出现概率
	5.返回前K个点出现频率最高的类别作为当前点预测分类
```
## 四、超参数和交叉验证
>**超参数 - distance**
~~~

~~~
![enter description here](./images/1574068741460.png)

>**交叉验证**
```
测试集非常宝贵，只能在最会的时候用它，当模型确定下来之后再去用测试集，而不是用测试集来调节参数。
先把原始数据切分成`训练集`和`测试集`，在训练集中找一份当成验证集。
```
![enter description here](./images/1574068854411.png)

>**k - 近邻**
- **第一部分**
```
	1.选取超参数的正确方法是：将原始训练集分为训练集和验证集，我们在验证集上尝试不同的超参数，最后保留表现最好那个.
	2.如果训练数据量不够，使用交叉验证方法，它能帮助我们在选取最优超参数的时候减少噪音。
	3.一旦找到最优的超参数，就让算法以该参数在测试集跑且只跑一次，并根据测试结果评价算法。
	4.最近邻分类器能够在CIFAR-10上得到将近40%的准确率。该算法简单易实现，但需要存储所有训练数据，并且在测试的时候过于耗费计算能力。
	5.最后，我们知道了仅仅使用L1和L2范数来进行像素比较是不够的，图像更多的是按照背景和颜色被分类，而不是语义主体分身。
```
- **第二部分**
```
	1.预处理你的数据：对你数据中的特征进行归一化（normalize），让其具有零平均值（zero mean）和单位方差（unit variance）。
	2.如果数据是高维数据，考虑使用降维方法，比如PCA。
	3..将数据随机分入训练集和验证集。按照一般规律，70%-90%数据作为训练集。
	4.在验证集上调优，尝试足够多的k值，尝试L1和L2两种范数计算方式。
```
## 五、线性分类
>**描述**

![](./images/1574070032750.png)
```
上面小猫图片下的数字：
	(32x32x3)：长 x 宽 x 颜色通道
	颜色通道：
		- 灰度图：1
		- 彩色图：3 （RGB）
	images：输入像素
	paramerters：权重参数（不止一个）
```
>**计算**

![](./images/1574070465225.png)
```
矩阵相乘
	W：10x3072	-	矩阵
	x：3072x1	-	矩阵
	b：10x1		-	矩阵
```
>**简述**

![enter description here](./images/1574070920127.png)
```
从上图可以看出，将小猫图片分为了 4 个像素点。
行和列一对一相乘，然后加起来
	· 0.2*56 + (-0.5*231) + 0.1*24 + 2.0*2 + 1.1 = -96.8
	· 1.5*56 + 1.3*231 + 2.1*24 + 0.0*2 + 3.2 = 437.9
	· 0*56 + 0.25*231 + 0.2*24 + (-0.3*2) + (-1.2) = 61.95
	
得分值高就是结果值，但这里明显不对，需要接下来的补充
```
## 六、损失函数
```
组合完之后相当于完成了一个线性分类的操作。
	W：权重参数，决策边界，倾斜度
	b：通过 b 找到与y轴的焦点值
```
![](./images/1574071740337.png)

>**损失函数表达式**

![](./images/1574071995006.png)
>**计算结果**

![](./images/1574072477370.png)
```
最红的损失函数是要将所有的样本都算出来
最终的损失值是跟样本的个数没有关系
```
![](./images/1574072587402.png)

## 七、正则化惩罚项

>**正则化表达式**

![enter description here](./images/1574073176757.png)
```
损失函数扩展	-	加惩罚项的损失函数
L = 损失函数 + 正则化惩罚项
```
>**损失函数终极版**

![enter description here](./images/1574073419778.png)

## 八、softmax 分类器
